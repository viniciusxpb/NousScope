{
    "perceptron": {
        "id": "perceptron",
        "arch": [
            1,
            1
        ],
        "activations": [
            "linear"
        ],
        "title": "ğŸ§  Perceptron",
        "year": "Frank Rosenblatt, 1958 - Cornell",
        "diagram": "x â”€â”€[w]â”€â”€â†’ y = wx + b",
        "description": "O avÃ´ de todas as redes neurais! Um Ãºnico neurÃ´nio que sÃ³ consegue aprender funÃ§Ãµes lineares.",
        "funFact": "ğŸ’¡ O Perceptron original era uma MÃQUINA FÃSICA com potenciÃ´metros como pesos!"
    },
    "xor": {
        "id": "xor",
        "arch": [
            1,
            2,
            1
        ],
        "activations": [
            "relu",
            "linear"
        ],
        "title": "âš¡ XOR Network",
        "year": "Minsky & Papert, 1969",
        "diagram": "x â”€â”€â†’[2]â”€â”€â†’ y",
        "description": "A rede que provou que um perceptron simples nÃ£o resolve XOR. Precisamos de camadas ocultas!",
        "funFact": "ğŸ’¡ O livro de Minsky causou o primeiro 'AI Winter' - uma dÃ©cada de descrenÃ§a em redes neurais."
    },
    "cybenko": {
        "id": "cybenko",
        "arch": [
            1,
            4,
            1
        ],
        "activations": [
            "sigmoid",
            "linear"
        ],
        "title": "ğŸŒŠ Universal Approximator",
        "year": "George Cybenko, 1989",
        "diagram": "x â”€â”€â†’[4Ïƒ]â”€â”€â†’ y",
        "description": "Teorema da AproximaÃ§Ã£o Universal: uma camada oculta com sigmoide pode aproximar QUALQUER funÃ§Ã£o contÃ­nua!",
        "funFact": "ğŸ’¡ O teorema nÃ£o diz QUANTOS neurÃ´nios vocÃª precisa - sÃ³ que Ã© possÃ­vel!"
    },
    "wide": {
        "id": "wide",
        "arch": [
            1,
            256,
            1
        ],
        "activations": [
            "relu",
            "linear"
        ],
        "title": "ğŸ“ Wide Network [256]",
        "year": "Arquitetura Moderna",
        "diagram": "x â”€â”€â†’[256]â”€â”€â†’ y",
        "description": "Redes largas tÃªm muita capacidade expressiva mas podem overfittar facilmente.",
        "funFact": "ğŸ’¡ Lottery Ticket Hypothesis: redes largas contÃªm sub-redes menores que funcionam tÃ£o bem!"
    },
    "deep": {
        "id": "deep",
        "arch": [
            1,
            4,
            4,
            4,
            4,
            4,
            4,
            4,
            4,
            1
        ],
        "activations": [
            "relu",
            "relu",
            "relu",
            "relu",
            "relu",
            "relu",
            "relu",
            "relu",
            "linear"
        ],
        "title": "ğŸ“š Deep Network [8Ã—4]",
        "year": "Deep Learning Era, 2010s",
        "diagram": "x â”€â”€â†’[4]â”€â”€â†’[4]â”€â”€â†’...â”€â”€â†’[4]â”€â”€â†’ y",
        "description": "Profundidade permite composiÃ§Ã£o hierÃ¡rquica de features, mas pode sofrer de vanishing gradients.",
        "funFact": "ğŸ’¡ ResNets resolveram vanishing gradients com skip connections!"
    },
    "pyramid": {
        "id": "pyramid",
        "arch": [
            1,
            8,
            4,
            2,
            1
        ],
        "activations": [
            "relu",
            "relu",
            "relu",
            "linear"
        ],
        "title": "ğŸ”º Pyramid",
        "year": "Encoder Architecture",
        "diagram": "x â”€â”€â†’[8]â”€â”€â†’[4]â”€â”€â†’[2]â”€â”€â†’ y",
        "description": "Arquitetura encoder: comprime informaÃ§Ã£o progressivamente. Comum em autoencoders.",
        "funFact": "ğŸ’¡ Funciona como um 'funil' que extrai features cada vez mais abstratas."
    },
    "hourglass": {
        "id": "hourglass",
        "arch": [
            1,
            2,
            4,
            8,
            4,
            2,
            1
        ],
        "activations": [
            "relu",
            "relu",
            "relu",
            "relu",
            "relu",
            "linear"
        ],
        "title": "â³ Hourglass",
        "year": "Autoencoder Architecture",
        "diagram": "x â”€â”€â†’[2]â”€â”€â†’[4]â”€â”€â†’[8]â”€â”€â†’[4]â”€â”€â†’[2]â”€â”€â†’ y",
        "description": "Expande para capturar detalhes, depois comprime. Usado em U-Nets para segmentaÃ§Ã£o.",
        "funFact": "ğŸ’¡ U-Net revolucionou segmentaÃ§Ã£o mÃ©dica com skip connections no hourglass!"
    }
}